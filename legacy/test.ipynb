{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1)), 'weight'), (Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1)), 'weight'), (Linear(in_features=400, out_features=120, bias=True), 'weight'), (Linear(in_features=120, out_features=84, bias=True), 'weight'), (Linear(in_features=84, out_features=10, bias=True), 'weight'))\n",
      "Sparsity in conv1.weight: 1.85%\n",
      "Global sparsity: 20.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # 5x5 image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "# model = LeNet()\n",
    "model = LeNet().to(device=device)\n",
    "parameters_to_prune = (\n",
    " (model.conv1, 'weight'),\n",
    " (model.conv2, 'weight'),\n",
    " (model.fc1, 'weight'),\n",
    " (model.fc2, 'weight'),\n",
    " (model.fc3, 'weight'),\n",
    ")\n",
    "print(parameters_to_prune)\n",
    "prune.global_unstructured(\n",
    " parameters_to_prune,\n",
    " pruning_method=prune.L1Unstructured,\n",
    "    amount=0.2,\n",
    ")\n",
    "# 计算卷积层和整个模型的稀疏度\n",
    "# 其实调用的是 Tensor.numel 内内函数，返回输入张量中元素的总数\n",
    "print(\n",
    " \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
    " 100. * float(torch.sum(model.conv1.weight == 0))\n",
    " / float(model.conv1.weight.nelement())\n",
    " )\n",
    ")\n",
    "zero_number = 0\n",
    "total_bumber = 0\n",
    "for name, m in model.named_modules():\n",
    "    if isinstance(m,torch.nn.Linear) or isinstance(m,torch.nn.Conv2d):\n",
    "        zero_number = zero_number + torch.sum(m.weight==0)\n",
    "        total_bumber = total_bumber + m.weight.numel()\n",
    "\n",
    "print(\n",
    " \"Global sparsity: {:.2f}%\".format(\n",
    " 100. * (zero_number/total_bumber)\n",
    " )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "Linear(in_features=400, out_features=120, bias=True)\n",
      "Linear(in_features=120, out_features=84, bias=True)\n",
      "Linear(in_features=84, out_features=10, bias=True)\n",
      "Sparsity in conv1.weight: 5.56%\n",
      "Global sparsity: 44.00%\n"
     ]
    }
   ],
   "source": [
    "for name, m in model.named_modules():\n",
    "    if isinstance(m,torch.nn.Linear) or isinstance(m,torch.nn.Conv2d):\n",
    "        if hasattr(m,\"weight_mask\"):\n",
    "            print(m)\n",
    "            m.weight.data = m.weight_orig\n",
    "\n",
    "prune.global_unstructured(\n",
    " parameters_to_prune,\n",
    " pruning_method=prune.L1Unstructured,\n",
    "    amount=0.3,\n",
    ")\n",
    "# 计算卷积层和整个模型的稀疏度\n",
    "# 其实调用的是 Tensor.numel 内内函数，返回输入张量中元素的总数\n",
    "print(\n",
    " \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
    " 100. * float(torch.sum(model.conv1.weight == 0))\n",
    " / float(model.conv1.weight.nelement())\n",
    " )\n",
    ")\n",
    "zero_number = 0\n",
    "total_bumber = 0\n",
    "for name, m in model.named_modules():\n",
    "    if isinstance(m,torch.nn.Linear) or isinstance(m,torch.nn.Conv2d):\n",
    "        zero_number = zero_number + torch.sum(m.weight==0)\n",
    "        total_bumber = total_bumber + m.weight.numel()\n",
    "\n",
    "print(\n",
    " \"Global sparsity: {:.2f}%\".format(\n",
    " 100. * (zero_number/total_bumber)\n",
    " )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LsqQuan(thd_pos=7, thd_neg=-8, s=1.620495319366455, per_channel=False)\n",
      "tensor([[[1.6205, 3.2410, 1.6205, 3.2410],\n",
      "         [1.6205, 3.2410, 0.0000, 3.2410],\n",
      "         [1.6205, 1.6205, 1.6205, 3.2410]],\n",
      "\n",
      "        [[3.2410, 1.6205, 0.0000, 3.2410],\n",
      "         [0.0000, 0.0000, 3.2410, 1.6205],\n",
      "         [3.2410, 1.6205, 3.2410, 1.6205]]], grad_fn=<MulBackward0>)\n",
      "Parameter containing:\n",
      "tensor([7, 7, 7, 7], dtype=torch.int32) Parameter containing:\n",
      "tensor([135, 135, 135, 135], dtype=torch.int32)\n",
      "tensor([[[1., 2., 1., 2.],\n",
      "         [1., 2., 0., 2.],\n",
      "         [1., 1., 1., 2.]],\n",
      "\n",
      "        [[2., 1., 0., 2.],\n",
      "         [0., 0., 2., 1.],\n",
      "         [2., 1., 2., 1.]]], grad_fn=<DivBackward0>)\n",
      "tensor([[[1., 2., 1., 2.],\n",
      "         [1., 2., 0., 2.],\n",
      "         [1., 1., 1., 2.]],\n",
      "\n",
      "        [[2., 1., 0., 2.],\n",
      "         [0., 0., 2., 1.],\n",
      "         [2., 1., 2., 1.]]], grad_fn=<TransposeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kang_you/anaconda3/envs/SNN/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from spike_quan_layer_IntegerOnly import PowerNormQuan,QInferAttention,QAttention,PowerNormInfer,LsqQuan\n",
    "from PowerNorm import MaskPowerNorm\n",
    "\n",
    "PN = MaskPowerNorm(num_features=4)\n",
    "x = torch.rand((2,3,4))*4\n",
    "x_quan_fn = LsqQuan(bit=4,per_channel=False)\n",
    "x_quan_fn.init_from(x)\n",
    "PN_quan_fn = LsqQuan(bit=4,per_channel=False)\n",
    "\n",
    "x_quan = x_quan_fn(x)\n",
    "\n",
    "print(x_quan_fn)\n",
    "print(x_quan)\n",
    "\n",
    "PNQuan = PowerNormQuan(m=PN,quan_fn=PN_quan_fn)\n",
    "output1 = PNQuan(x_quan)\n",
    "\n",
    "output1 = PNQuan(x_quan)\n",
    "\n",
    "PNQuanInfer = PowerNormInfer(m=PNQuan,last_act_quan=x_quan_fn)\n",
    "print(PNQuanInfer.N1,PNQuanInfer.M1)\n",
    "output2 = PNQuanInfer(x_quan/x_quan_fn.s)\n",
    "\n",
    "print(output1/PNQuan.quan_fn.s)\n",
    "print(output2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quan_x LsqQuan(thd_pos=7, thd_neg=-8, s=1.4090669638576874, per_channel=False)\n",
      "quan_qkv_weight LsqQuan(thd_pos=7, thd_neg=-8, s=0.19660905948470428, per_channel=False)\n",
      "quan_proj_weight LsqQuan(thd_pos=7, thd_neg=-8, s=0.17160493360573897, per_channel=False)\n",
      "scale 0.7071067811865476\n",
      "quan_q LsqQuan(thd_pos=7, thd_neg=-8, s=1.0474673340955825, per_channel=False)\n",
      "quan_k LsqQuan(thd_pos=7, thd_neg=-8, s=0.9033111870689389, per_channel=False)\n",
      "quan_v LsqQuan(thd_pos=7, thd_neg=-8, s=0.8982452689186882, per_channel=False)\n",
      "quan_proj LsqQuan(thd_pos=7, thd_neg=-8, s=0.30580500484626777, per_channel=False)\n",
      "attn_quan LsqQuan(thd_pos=7, thd_neg=-8, s=0.3828836410068537, per_channel=False)\n",
      "after_attn_quan LsqQuan(thd_pos=7, thd_neg=-8, s=0.3365168127666457, per_channel=False)\n",
      "tensor(2., grad_fn=<SumBackward0>)\n",
      "=====================QAtten.q, QAttenInfer.q======================\n",
      "Parameter containing:\n",
      "tensor(192) Parameter containing:\n",
      "tensor(10)\n",
      "tensor([[[[ 1.1221, -1.4961],\n",
      "          [ 0.7481, -1.3091],\n",
      "          [ 1.1221, -0.1870]],\n",
      "\n",
      "         [[ 0.9351,  1.1221],\n",
      "          [ 0.5610,  1.4961],\n",
      "          [ 0.9351,  0.3740]]]], grad_fn=<DivBackward0>)\n",
      "==================================================================\n",
      "tensor([[[[ 6., -8.],\n",
      "          [ 4., -7.],\n",
      "          [ 6., -1.]],\n",
      "\n",
      "         [[ 5.,  6.],\n",
      "          [ 3.,  8.],\n",
      "          [ 5.,  2.]]]], grad_fn=<UnbindBackward0>)\n",
      "tensor([[[[ 1.1221, -1.4961],\n",
      "          [ 0.7481, -1.3091],\n",
      "          [ 1.1221, -0.1870]],\n",
      "\n",
      "         [[ 0.9351,  1.1221],\n",
      "          [ 0.5610,  1.4961],\n",
      "          [ 0.9351,  0.3740]]]], grad_fn=<DivBackward0>)\n",
      "tensor([[[[ 1.1250, -1.5000],\n",
      "          [ 0.7500, -1.3125],\n",
      "          [ 1.1250, -0.1875]],\n",
      "\n",
      "         [[ 0.9375,  1.1250],\n",
      "          [ 0.5625,  1.5000],\n",
      "          [ 0.9375,  0.3750]]]], grad_fn=<DivBackward0>)\n",
      "==================================================================\n",
      "tensor([[[[ 1., -1.],\n",
      "          [ 1., -1.],\n",
      "          [ 1.,  0.]],\n",
      "\n",
      "         [[ 1.,  1.],\n",
      "          [ 1.,  1.],\n",
      "          [ 1.,  0.]]]], grad_fn=<DivBackward0>)\n",
      "tensor([[[[ 1., -2.],\n",
      "          [ 1., -1.],\n",
      "          [ 1., -0.]],\n",
      "\n",
      "         [[ 1.,  1.],\n",
      "          [ 1.,  2.],\n",
      "          [ 1.,  0.]]]], grad_fn=<RoundBackward0>)\n",
      "==================================================================\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "tensor(8.8818e-16, grad_fn=<SumBackward0>)\n",
      "tensor(7., grad_fn=<SumBackward0>)\n",
      "tensor(2., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from spike_quan_layer_IntegerOnly import QInferAttention,QAttention,LsqQuan\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "while(1):\n",
    "    x = torch.rand((1,3,4))*4\n",
    "    quan_x = LsqQuan(bit=4,per_channel=False)\n",
    "    quan_x.init_from(x)\n",
    "    x = quan_x(x)\n",
    "\n",
    "    quan_qkv_weight = LsqQuan(bit=4,per_channel=False)\n",
    "    quan_proj_weight = LsqQuan(bit=4,per_channel=False)\n",
    "    quan_q = LsqQuan(bit=4,per_channel=False)\n",
    "    quan_k = LsqQuan(bit=4,per_channel=False)\n",
    "    quan_v = LsqQuan(bit=4,per_channel=False)\n",
    "    quan_proj = LsqQuan(bit=4,per_channel=False)\n",
    "    attn_quan = LsqQuan(bit=4,per_channel=False)\n",
    "    after_attn_quan = LsqQuan(bit=4,per_channel=False)\n",
    "\n",
    "    QAtten = QAttention(dim=4,quan_qkv_weight=quan_qkv_weight,quan_proj_weight=quan_proj_weight,quan_q=quan_q,quan_k=quan_k,quan_v=quan_v,quan_proj=quan_proj,attn_quan=attn_quan,after_attn_quan=after_attn_quan,num_heads=2)\n",
    "\n",
    "    # initialize quantize attention\n",
    "    output = QAtten(x)\n",
    "    # print(\"quan_qkv_weight\",QAtten.quan_qkv_weight)\n",
    "    # print(\"quan_proj_weight\",QAtten.quan_proj_weight)\n",
    "    # print(\"quan_q\",QAtten.quan_q)\n",
    "    # print(\"quan_k\",QAtten.quan_k)\n",
    "    # print(\"quan_v\",QAtten.quan_v)\n",
    "    # print(\"quan_proj\",QAtten.quan_proj)\n",
    "    # print(\"attn_quan\",QAtten.attn_quan)\n",
    "    # print(\"after_attn_quan\",QAtten.after_attn_quan)\n",
    "\n",
    "    QAttenInfer = QInferAttention(m=QAtten, last_act_quan=quan_x)\n",
    "\n",
    "    # Compare output:\n",
    "    output1 = QAtten(x)/QAtten.quan_proj.s\n",
    "    output2 = QAttenInfer(x/quan_x.s)\n",
    "\n",
    "    if torch.abs(QAtten.q - QAttenInfer.q).sum() >= 1:\n",
    "        print(\"quan_x\",quan_x)\n",
    "        print(\"quan_qkv_weight\",QAtten.quan_qkv_weight)\n",
    "        print(\"quan_proj_weight\",QAtten.quan_proj_weight)\n",
    "        print(\"scale\",QAtten.scale)\n",
    "        print(\"quan_q\",QAtten.quan_q)\n",
    "        print(\"quan_k\",QAtten.quan_k)\n",
    "        print(\"quan_v\",QAtten.quan_v)\n",
    "        print(\"quan_proj\",QAtten.quan_proj)\n",
    "        print(\"attn_quan\",QAtten.attn_quan)\n",
    "        print(\"after_attn_quan\",QAtten.after_attn_quan)\n",
    "        print(torch.abs(QAtten.q - QAttenInfer.q).sum())\n",
    "        print(\"=====================QAtten.q, QAttenInfer.q======================\")\n",
    "        print(QAttenInfer.neuron_q_M,QAttenInfer.neuron_q_N)\n",
    "        print(QAtten.q1/quan_q.s)\n",
    "        print(\"==================================================================\")\n",
    "        print(QAttenInfer.q1)\n",
    "        print(QAttenInfer.q1*quan_x.s*QAtten.quan_qkv_weight.s*QAtten.scale/quan_q.s)\n",
    "        print(QAttenInfer.q1*QAttenInfer.neuron_q_M/(2**QAttenInfer.neuron_q_N))\n",
    "        print(\"==================================================================\")\n",
    "        print(QAtten.q)\n",
    "        print(QAttenInfer.q)\n",
    "        print(\"==================================================================\")\n",
    "        print(torch.abs(QAtten.k - QAttenInfer.k).sum())\n",
    "        print(torch.abs(QAtten.v - QAttenInfer.v).sum())\n",
    "        print(torch.abs(QAtten.attn - QAttenInfer.attn).sum())\n",
    "        print(torch.abs(output1 - output2).sum())\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True],\n",
      "         [True, True, True, True, True, True, True, True, True, True, True,\n",
      "          True, True, True, True, True]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from spike_quan_layer_IntegerOnly import AdditionInfer,AdditionQuan,LsqQuan\n",
    "\n",
    "x1 = torch.rand((1,16,16))*4\n",
    "x2 = torch.rand((1,16,16))*4\n",
    "\n",
    "x1_quan_fn = LsqQuan(bit=4,per_channel=False)\n",
    "x2_quan_fn = LsqQuan(bit=4,per_channel=False)\n",
    "\n",
    "x1_quan_fn.init_from(x1)\n",
    "x2_quan_fn.init_from(x2)\n",
    "\n",
    "x1_quan = x1_quan_fn(x1)\n",
    "x2_quan = x2_quan_fn(x2)\n",
    "\n",
    "AddQuan = AdditionQuan(quan_a_fn=LsqQuan(bit=4,per_channel=False))\n",
    "\n",
    "output1 = AddQuan(x1_quan,x2_quan)\n",
    "output1 = AddQuan(x1_quan,x2_quan)/AddQuan.quan_a_fn.s\n",
    "\n",
    "AddInfer = AdditionInfer(m=AddQuan,quan_input1_fn=x1_quan_fn,quan_input2_fn=x2_quan_fn)\n",
    "output2 = AddInfer(x1_quan/x1_quan_fn.s,x2_quan/x2_quan_fn.s)\n",
    "\n",
    "print(output1==output2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.2620, 0.8055, 0.6904, 0.4581], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2620, 0.8055, 0.6904, 0.4581], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0708, 0.6759, 0.6418, 0.8788], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0708, 0.6759, 0.6418, 0.8788], requires_grad=True)\n",
      "tensor([[[0.2103, 0.5787, 0.7762, 0.5156],\n",
      "         [0.0825, 0.4825, 0.6976, 0.4493],\n",
      "         [0.9010, 0.0231, 0.0402, 0.4332],\n",
      "         [0.5531, 0.5815, 0.2986, 0.5273]]])\n",
      "tensor([[[-0.3289,  0.9079,  1.5119,  0.8684],\n",
      "         [-0.3386,  0.8745,  1.4838,  0.9230],\n",
      "         [ 0.4742, -0.0577,  0.0459,  0.9860],\n",
      "         [ 0.2178,  1.3318, -0.5364,  1.0305]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.2754,  0.8768,  1.3953,  0.8698],\n",
      "         [-0.2838,  0.8479,  1.3710,  0.9171],\n",
      "         [ 0.4202,  0.0406,  0.1257,  0.9716],\n",
      "         [ 0.1981,  1.2439, -0.3786,  1.0102]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from spike_quan_layer import MyLayerNorm\n",
    "import torch\n",
    "\n",
    "LN = torch.nn.LayerNorm(normalized_shape=4,elementwise_affine=True,eps=1e-6).eval()\n",
    "MyLN = MyLayerNorm(dim=4,eps=1e-6).eval()\n",
    "\n",
    "LN.weight.data = torch.rand(LN.weight.data.shape)\n",
    "LN.bias.data = torch.rand(LN.bias.data.shape)\n",
    "MyLN.weight.data = LN.weight.data\n",
    "MyLN.bias.data = LN.bias.data\n",
    "print(LN.weight)\n",
    "print(MyLN.weight)\n",
    "print(LN.bias)\n",
    "print(MyLN.bias)\n",
    "\n",
    "x = torch.rand((1,4,4))\n",
    "print(x)\n",
    "output = LN(x)\n",
    "output1 = MyLN(x)\n",
    "\n",
    "print(output)\n",
    "print(output1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.global_pool False\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "运算量：4.249G, 参数量：21.975M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "运算量：3.679G, 参数量：21.798M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "from models_vit import vit_small_patch16\n",
    "from torchvision import models\n",
    "\n",
    "vit_small = vit_small_patch16()\n",
    "vit_small.eval()\n",
    "\n",
    "input = torch.randn(1, 3, 224, 224)  # 随机生成一个输入张量，这个尺寸应该与模型输入的尺寸相匹配\n",
    "flops, params = profile(vit_small, inputs=(input,))\n",
    "\n",
    "# 将结果转换为更易于阅读的格式\n",
    "flops, params = clever_format([flops, params], '%.3f')\n",
    "\n",
    "print(f\"运算量：{flops}, 参数量：{params}\")\n",
    "\n",
    "vit_small = models.resnet34()\n",
    "vit_small.eval()\n",
    "\n",
    "input = torch.randn(1, 3, 224, 224)  # 随机生成一个输入张量，这个尺寸应该与模型输入的尺寸相匹配\n",
    "flops, params = profile(vit_small, inputs=(input,))\n",
    "\n",
    "# 将结果转换为更易于阅读的格式\n",
    "flops, params = clever_format([flops, params], '%.3f')\n",
    "\n",
    "print(f\"运算量：{flops}, 参数量：{params}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.global_pool False\n",
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block(\n",
      "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Linear(in_features=384, out_features=1000, bias=True)\n",
      ")\n",
      "\n",
      "patch_embed\n",
      "patch_embed.proj\n",
      "pos_drop\n",
      "blocks\n",
      "blocks.0\n",
      "blocks.0.norm1\n",
      "blocks.0.attn\n",
      "blocks.0.attn.qkv\n",
      "blocks.0.attn.attn_drop\n",
      "blocks.0.attn.proj\n",
      "blocks.0.attn.proj_drop\n",
      "blocks.0.drop_path\n",
      "blocks.0.norm2\n",
      "blocks.0.mlp\n",
      "blocks.0.mlp.fc1\n",
      "blocks.0.mlp.act\n",
      "blocks.0.mlp.fc2\n",
      "blocks.0.mlp.drop\n",
      "blocks.1\n",
      "blocks.1.norm1\n",
      "blocks.1.attn\n",
      "blocks.1.attn.qkv\n",
      "blocks.1.attn.attn_drop\n",
      "blocks.1.attn.proj\n",
      "blocks.1.attn.proj_drop\n",
      "blocks.1.drop_path\n",
      "blocks.1.norm2\n",
      "blocks.1.mlp\n",
      "blocks.1.mlp.fc1\n",
      "blocks.1.mlp.act\n",
      "blocks.1.mlp.fc2\n",
      "blocks.1.mlp.drop\n",
      "blocks.2\n",
      "blocks.2.norm1\n",
      "blocks.2.attn\n",
      "blocks.2.attn.qkv\n",
      "blocks.2.attn.attn_drop\n",
      "blocks.2.attn.proj\n",
      "blocks.2.attn.proj_drop\n",
      "blocks.2.drop_path\n",
      "blocks.2.norm2\n",
      "blocks.2.mlp\n",
      "blocks.2.mlp.fc1\n",
      "blocks.2.mlp.act\n",
      "blocks.2.mlp.fc2\n",
      "blocks.2.mlp.drop\n",
      "blocks.3\n",
      "blocks.3.norm1\n",
      "blocks.3.attn\n",
      "blocks.3.attn.qkv\n",
      "blocks.3.attn.attn_drop\n",
      "blocks.3.attn.proj\n",
      "blocks.3.attn.proj_drop\n",
      "blocks.3.drop_path\n",
      "blocks.3.norm2\n",
      "blocks.3.mlp\n",
      "blocks.3.mlp.fc1\n",
      "blocks.3.mlp.act\n",
      "blocks.3.mlp.fc2\n",
      "blocks.3.mlp.drop\n",
      "blocks.4\n",
      "blocks.4.norm1\n",
      "blocks.4.attn\n",
      "blocks.4.attn.qkv\n",
      "blocks.4.attn.attn_drop\n",
      "blocks.4.attn.proj\n",
      "blocks.4.attn.proj_drop\n",
      "blocks.4.drop_path\n",
      "blocks.4.norm2\n",
      "blocks.4.mlp\n",
      "blocks.4.mlp.fc1\n",
      "blocks.4.mlp.act\n",
      "blocks.4.mlp.fc2\n",
      "blocks.4.mlp.drop\n",
      "blocks.5\n",
      "blocks.5.norm1\n",
      "blocks.5.attn\n",
      "blocks.5.attn.qkv\n",
      "blocks.5.attn.attn_drop\n",
      "blocks.5.attn.proj\n",
      "blocks.5.attn.proj_drop\n",
      "blocks.5.drop_path\n",
      "blocks.5.norm2\n",
      "blocks.5.mlp\n",
      "blocks.5.mlp.fc1\n",
      "blocks.5.mlp.act\n",
      "blocks.5.mlp.fc2\n",
      "blocks.5.mlp.drop\n",
      "blocks.6\n",
      "blocks.6.norm1\n",
      "blocks.6.attn\n",
      "blocks.6.attn.qkv\n",
      "blocks.6.attn.attn_drop\n",
      "blocks.6.attn.proj\n",
      "blocks.6.attn.proj_drop\n",
      "blocks.6.drop_path\n",
      "blocks.6.norm2\n",
      "blocks.6.mlp\n",
      "blocks.6.mlp.fc1\n",
      "blocks.6.mlp.act\n",
      "blocks.6.mlp.fc2\n",
      "blocks.6.mlp.drop\n",
      "blocks.7\n",
      "blocks.7.norm1\n",
      "blocks.7.attn\n",
      "blocks.7.attn.qkv\n",
      "blocks.7.attn.attn_drop\n",
      "blocks.7.attn.proj\n",
      "blocks.7.attn.proj_drop\n",
      "blocks.7.drop_path\n",
      "blocks.7.norm2\n",
      "blocks.7.mlp\n",
      "blocks.7.mlp.fc1\n",
      "blocks.7.mlp.act\n",
      "blocks.7.mlp.fc2\n",
      "blocks.7.mlp.drop\n",
      "blocks.8\n",
      "blocks.8.norm1\n",
      "blocks.8.attn\n",
      "blocks.8.attn.qkv\n",
      "blocks.8.attn.attn_drop\n",
      "blocks.8.attn.proj\n",
      "blocks.8.attn.proj_drop\n",
      "blocks.8.drop_path\n",
      "blocks.8.norm2\n",
      "blocks.8.mlp\n",
      "blocks.8.mlp.fc1\n",
      "blocks.8.mlp.act\n",
      "blocks.8.mlp.fc2\n",
      "blocks.8.mlp.drop\n",
      "blocks.9\n",
      "blocks.9.norm1\n",
      "blocks.9.attn\n",
      "blocks.9.attn.qkv\n",
      "blocks.9.attn.attn_drop\n",
      "blocks.9.attn.proj\n",
      "blocks.9.attn.proj_drop\n",
      "blocks.9.drop_path\n",
      "blocks.9.norm2\n",
      "blocks.9.mlp\n",
      "blocks.9.mlp.fc1\n",
      "blocks.9.mlp.act\n",
      "blocks.9.mlp.fc2\n",
      "blocks.9.mlp.drop\n",
      "blocks.10\n",
      "blocks.10.norm1\n",
      "blocks.10.attn\n",
      "blocks.10.attn.qkv\n",
      "blocks.10.attn.attn_drop\n",
      "blocks.10.attn.proj\n",
      "blocks.10.attn.proj_drop\n",
      "blocks.10.drop_path\n",
      "blocks.10.norm2\n",
      "blocks.10.mlp\n",
      "blocks.10.mlp.fc1\n",
      "blocks.10.mlp.act\n",
      "blocks.10.mlp.fc2\n",
      "blocks.10.mlp.drop\n",
      "blocks.11\n",
      "blocks.11.norm1\n",
      "blocks.11.attn\n",
      "blocks.11.attn.qkv\n",
      "blocks.11.attn.attn_drop\n",
      "blocks.11.attn.proj\n",
      "blocks.11.attn.proj_drop\n",
      "blocks.11.drop_path\n",
      "blocks.11.norm2\n",
      "blocks.11.mlp\n",
      "blocks.11.mlp.fc1\n",
      "blocks.11.mlp.act\n",
      "blocks.11.mlp.fc2\n",
      "blocks.11.mlp.drop\n",
      "norm\n",
      "head\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from models_vit import vit_small_patch16\n",
    "\n",
    "vit_small = vit_small_patch16()\n",
    "vit_small.eval()\n",
    "\n",
    "print(vit_small)\n",
    "\n",
    "input_shape = {}\n",
    "output_shape = {}\n",
    "\n",
    "def layer_hook(module, inp, out):\n",
    "    global input_shape, output_shape\n",
    "    input_shape[module] = inp[0].shape\n",
    "    output_shape[module] = out.shape\n",
    "\n",
    "for name,module in list(vit_small.named_modules()):\n",
    "    if isinstance(module,torch.nn.Conv2d):\n",
    "        module.register_forward_hook(layer_hook)\n",
    "    if isinstance(module,torch.nn.Linear):\n",
    "        module.register_forward_hook(layer_hook)\n",
    "\n",
    "input = torch.rand((1,3,224,224))\n",
    "\n",
    "output = vit_small(input)\n",
    "Head = 6\n",
    "\n",
    "idx = 0\n",
    "for name,module in list(vit_small.named_modules()):\n",
    "    print(name)\n",
    "    if isinstance(module,torch.nn.Linear) and name.count(\"qkv\") > 0:\n",
    "        for i in range(3):\n",
    "            file = open(f\"forEyeriss/{str(idx).rjust(2,'0')}.yaml\",\"w+\")\n",
    "            file.write(\"{{include_text('../problem_base.yaml')}}\\n\")\n",
    "            file.write(\"problem:\\n\")\n",
    "            file.write(\"  <<<: *problem_base\\n\")\n",
    "            inputShape = input_shape[module]\n",
    "            outputShape = output_shape[module]\n",
    "            file.write(f\"  instance: {{C: {inputShape[-1]}, M: {outputShape[-1]//3}, P: {outputShape[-2]}}}\\n\")\n",
    "            idx = idx + 1\n",
    "            file.close()\n",
    "        file = open(f\"forEyeriss/{str(idx).rjust(2,'0')}.yaml\",\"w+\")\n",
    "        file.write(\"{{include_text('../problem_base.yaml')}}\\n\")\n",
    "        file.write(\"problem:\\n\")\n",
    "        file.write(\"  <<<: *problem_base\\n\")\n",
    "        inputShape = input_shape[module]\n",
    "        outputShape = output_shape[module]\n",
    "        file.write(f\"  instance: {{M: {inputShape[-1]//Head}, C: {outputShape[-2]}, P: {Head}}}\\n\")\n",
    "        idx = idx + 1\n",
    "        file.close()\n",
    "\n",
    "        file = open(f\"forEyeriss/{str(idx).rjust(2,'0')}.yaml\",\"w+\")\n",
    "        file.write(\"{{include_text('../problem_base.yaml')}}\\n\")\n",
    "        file.write(\"problem:\\n\")\n",
    "        file.write(\"  <<<: *problem_base\\n\")\n",
    "        inputShape = input_shape[module]\n",
    "        outputShape = output_shape[module]\n",
    "        file.write(f\"  instance: {{M: {outputShape[-2]}, C: {outputShape[-2]}, P: {Head}}}\\n\")\n",
    "        idx = idx + 1\n",
    "        file.close()\n",
    "    elif isinstance(module,torch.nn.Linear):\n",
    "        file = open(f\"forEyeriss/{str(idx).rjust(2,'0')}.yaml\",\"w+\")\n",
    "        file.write(\"{{include_text('../problem_base.yaml')}}\\n\")\n",
    "        file.write(\"problem:\\n\")\n",
    "        file.write(\"  <<<: *problem_base\\n\")\n",
    "        inputShape = input_shape[module]\n",
    "        outputShape = output_shape[module]\n",
    "        file.write(f\"  instance: {{C: {inputShape[-1]}, M: {outputShape[-1]}, P: {outputShape[-2]}}}\\n\")\n",
    "        idx = idx + 1\n",
    "        file.close()\n",
    "    elif isinstance(module,torch.nn.Conv2d):\n",
    "        file = open(f\"forEyeriss/{str(idx).rjust(2,'0')}.yaml\",\"w+\")\n",
    "        file.write(\"{{include_text('../problem_base.yaml')}}\\n\")\n",
    "        file.write(\"problem:\\n\")\n",
    "        file.write(\"  <<<: *problem_base\\n\")\n",
    "        outputShape = output_shape[module]\n",
    "        file.write(f\"  instance: {{C: {module.in_channels}, M: {module.out_channels}, P: {outputShape[-1]}, Q: {outputShape[-2]}, R: {module.kernel_size[0]}, S: {module.kernel_size[1]}, HStride: {module.stride[0]}, WStride: {module.stride[0]}}}\\n\")\n",
    "        idx = idx + 1\n",
    "        file.close()\n",
    "    # if isinstance(module,torch.nn.Linear):\n",
    "    #     file = open(f\"forEyeriss/{str(idx).rjust(2,'0')}.yaml\",\"w+\")\n",
    "    #     file.write(\"{{include_text('../problem_base.yaml')}}\\n\")\n",
    "    #     file.write(\"problem:\\n\")\n",
    "    #     file.write(\"  <<<: *problem_base\\n\")\n",
    "    #     inputShape = input_shape[module]\n",
    "    #     outputShape = output_shape[module]\n",
    "    #     file.write(f\"  instance: {{C: {inputShape[-1]}, M: {outputShape[-1]}}}\\n\")\n",
    "    #     idx = idx + 1\n",
    "    #     file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.global_pool False\n",
      "Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16)) tensor(1204224.)\n",
      "Linear(in_features=384, out_features=1152, bias=True) tensor(1809408.)\n",
      "Linear(in_features=384, out_features=384, bias=True) tensor(2414592.)\n",
      "Linear(in_features=384, out_features=1536, bias=True) tensor(3019776.)\n",
      "Linear(in_features=1536, out_features=384, bias=True) tensor(5440512.)\n",
      "Linear(in_features=384, out_features=1152, bias=True) tensor(6045696.)\n",
      "Linear(in_features=384, out_features=384, bias=True) tensor(6650880.)\n",
      "Linear(in_features=384, out_features=1536, bias=True) tensor(7256064.)\n",
      "Linear(in_features=1536, out_features=384, bias=True) tensor(9676800.)\n",
      "Linear(in_features=384, out_features=1152, bias=True) tensor(10281984.)\n",
      "Linear(in_features=384, out_features=384, bias=True) tensor(10887168.)\n",
      "Linear(in_features=384, out_features=1536, bias=True) tensor(11492352.)\n",
      "Linear(in_features=1536, out_features=384, bias=True) tensor(13913088.)\n",
      "Linear(in_features=384, out_features=1152, bias=True) tensor(14518272.)\n",
      "Linear(in_features=384, out_features=384, bias=True) tensor(15123456.)\n",
      "Linear(in_features=384, out_features=1536, bias=True) tensor(15728640.)\n",
      "Linear(in_features=1536, out_features=384, bias=True) tensor(18149376.)\n",
      "Linear(in_features=384, out_features=1152, bias=True) tensor(18754560.)\n",
      "Linear(in_features=384, out_features=384, bias=True) tensor(19359744.)\n",
      "Linear(in_features=384, out_features=1536, bias=True) tensor(19964928.)\n",
      "Linear(in_features=1536, out_features=384, bias=True) tensor(22385664.)\n",
      "Linear(in_features=384, out_features=1152, bias=True) tensor(22990848.)\n",
      "Linear(in_features=384, out_features=384, bias=True) tensor(23596032.)\n",
      "Linear(in_features=384, out_features=1536, bias=True) tensor(24201216.)\n",
      "Linear(in_features=1536, out_features=384, bias=True) tensor(26621952.)\n",
      "Linear(in_features=384, out_features=1152, bias=True) tensor(27227136.)\n",
      "Linear(in_features=384, out_features=384, bias=True) tensor(27832320.)\n",
      "Linear(in_features=384, out_features=1536, bias=True) tensor(28437504.)\n",
      "Linear(in_features=1536, out_features=384, bias=True) tensor(30858240.)\n",
      "Linear(in_features=384, out_features=1152, bias=True) tensor(31463424.)\n",
      "Linear(in_features=384, out_features=384, bias=True) tensor(32068608.)\n",
      "Linear(in_features=384, out_features=1536, bias=True) tensor(32673792.)\n",
      "Linear(in_features=1536, out_features=384, bias=True) tensor(35094528.)\n",
      "Linear(in_features=384, out_features=1152, bias=True) tensor(35699712.)\n",
      "Linear(in_features=384, out_features=384, bias=True) tensor(36304896.)\n",
      "Linear(in_features=384, out_features=1536, bias=True) tensor(36910080.)\n",
      "Linear(in_features=1536, out_features=384, bias=True) tensor(39330816.)\n",
      "Linear(in_features=384, out_features=1152, bias=True) tensor(39936000.)\n",
      "Linear(in_features=384, out_features=384, bias=True) tensor(40541184.)\n",
      "Linear(in_features=384, out_features=1536, bias=True) tensor(41146368.)\n",
      "Linear(in_features=1536, out_features=384, bias=True) tensor(43567104.)\n",
      "Linear(in_features=384, out_features=1152, bias=True) tensor(44172288.)\n",
      "Linear(in_features=384, out_features=384, bias=True) tensor(44777472.)\n",
      "Linear(in_features=384, out_features=1536, bias=True) tensor(45382656.)\n",
      "Linear(in_features=1536, out_features=384, bias=True) tensor(47803392.)\n",
      "Linear(in_features=384, out_features=1152, bias=True) tensor(48408576.)\n",
      "Linear(in_features=384, out_features=384, bias=True) tensor(49013760.)\n",
      "Linear(in_features=384, out_features=1536, bias=True) tensor(49618944.)\n",
      "Linear(in_features=1536, out_features=384, bias=True) tensor(52039680.)\n",
      "Linear(in_features=384, out_features=1000, bias=True) tensor(52042752.)\n",
      "communication_traffic 6.2039794921875 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "import torchvision\n",
    "from models_vit import vit_small_patch16\n",
    "\n",
    "vit_small = vit_small_patch16()\n",
    "\n",
    "communication_traffic = 0.0\n",
    "\n",
    "def hook_input_feature_size(module,input,output):\n",
    "    global communication_traffic\n",
    "    if module.in_features == 384 and module.out_features == 384:\n",
    "    communication_traffic = communication_traffic + torch.prod(torch.tensor(input[0].shape))*8\n",
    "    print(module,communication_traffic)\n",
    "\n",
    "for module in vit_small.modules():\n",
    "    if isinstance(module,torch.nn.Linear) or isinstance(module,torch.nn.Conv2d) or isinstance(module,torch.nn.MaxPool2d) or isinstance(module,torch.nn.AdaptiveAvgPool2d):\n",
    "        module.register_forward_hook(hook_input_feature_size)\n",
    "\n",
    "x = torch.rand(1,3,224,224)\n",
    "\n",
    "out = vit_small(x)\n",
    "print(\"communication_traffic\",(communication_traffic/(1024*1024*8)).item(),\"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from spike_quan_layer import spiking_softmax\n",
    "\n",
    "x_list = []\n",
    "x_accu = 0.0\n",
    "T = 4\n",
    "for i in range(T):\n",
    "    x_list.append(torch.rand(2,12))\n",
    "    x_accu = x_accu + x_list[-1]\n",
    "ssoftmax = spiking_softmax(T=4)\n",
    "\n",
    "y1 = torch.nn.functional.softmax(dim=-1)\n",
    "y2 = 0.0\n",
    "for i in range(T):\n",
    "    y2 = y2 + ssoftmax(x_list[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "QANN = torch.load(\"QANN_PATCH_BeforeNorm.pth\")\n",
    "SNN = torch.load(\"SNN_PATCH_BeforeNorm.pth\").reshape(24,4,128,56,56)\n",
    "\n",
    "print(QANN.abs().mean())\n",
    "\n",
    "\n",
    "print(SNN.sum(dim=0).abs().mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kang_you/SpikeZIP_transformer_Hybrid/spike_quan_layer.py:1549: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.alpha = nn.Parameter(torch.tensor(torch.ones(1) * init_alpha))\n",
      "/home/kang_you/SpikeZIP_transformer_Hybrid/spike_quan_layer.py:1550: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.gamma = nn.Parameter(torch.tensor(torch.ones(C)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 8\n",
      "Parameter containing:\n",
      "tensor([1.], device='cuda:0', requires_grad=True) Parameter containing:\n",
      "tensor([0.2500], device='cuda:0', requires_grad=True) Parameter containing:\n",
      "tensor(1., device='cuda:0', requires_grad=True)\n",
      "myquan:s_scale= tensor(1., device='cuda:0', grad_fn=<AddBackward0>)\n",
      "myquan:x/s_scale= tensor([[[0.7803],\n",
      "         [0.9474],\n",
      "         [1.4505],\n",
      "         [0.7960],\n",
      "         [1.4146],\n",
      "         [0.7544],\n",
      "         [1.4963],\n",
      "         [0.6601]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor([[[0.0701],\n",
      "         [0.1118],\n",
      "         [0.2376],\n",
      "         [0.0740],\n",
      "         [0.2286],\n",
      "         [0.0636],\n",
      "         [0.2491],\n",
      "         [0.0400]]], device='cuda:0', grad_fn=<MulBackward0>) tensor([[[0.],\n",
      "         [0.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [1.],\n",
      "         [0.]]], device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from spike_quan_layer import DyHT, MyQuan\n",
    "\n",
    "\n",
    "x = torch.rand(size=(1,8,1)).cuda()\n",
    "mydyht = DyHT(C=1, init_alpha=0.25).cuda()\n",
    "myquan = MyQuan(level=8, sym=True).cuda()\n",
    "myquan.init_state = myquan.batch_init\n",
    "myquan.s = mydyht.gamma/myquan.pos_max\n",
    "print(mydyht.gamma, mydyht.alpha, myquan.s)\n",
    "y1 = mydyht(x)\n",
    "y2 = myquan(x * mydyht.gamma * mydyht.alpha)\n",
    "\n",
    "print(y1,y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
